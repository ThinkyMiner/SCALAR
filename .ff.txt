import numpy as np

# Function to load the dataset (IRIS dataset excluding the label column)
def load_iris_data(file_path):
    data = np.loadtxt(file_path, delimiter=',', skiprows=1, usecols=(0, 1, 2, 3))
    return data

# Function to initialize centroids by randomly selecting K points
def initialize_centroids(X, K):
    np.random.seed(42)  # for reproducibility
    indices = np.random.choice(X.shape[0], K, replace=False)
    return X[indices]

# Function to compute Euclidean distance between points and centroids
def compute_distances(X, centroids):
    distances = np.zeros((X.shape[0], centroids.shape[0]))
    for i, centroid in enumerate(centroids):
        distances[:, i] = np.linalg.norm(X - centroid, axis=1)
    return distances

# Function to assign clusters based on closest centroid
def assign_clusters(distances):
    return np.argmin(distances, axis=1)

# Function to update centroids by calculating the mean of assigned points
def update_centroids(X, labels, K):
    new_centroids = np.zeros((K, X.shape[1]))
    for k in range(K):
        new_centroids[k] = np.mean(X[labels == k], axis=0)
    return new_centroids

# Function to implement K-means algorithm
def kmeans(X, K, max_iters=100, tol=1e-4):
    centroids = initialize_centroids(X, K)
    for _ in range(max_iters):
        old_centroids = centroids.copy()
        distances = compute_distances(X, centroids)
        labels = assign_clusters(distances)
        centroids = update_centroids(X, labels, K)
        if np.linalg.norm(centroids - old_centroids) < tol:
            break
    return centroids, labels

# Function to select closest samples to centroid
def select_closest_samples(X, labels, centroids, num_samples=25):
    selected_samples = []
    selected_labels = []
    for k in range(centroids.shape[0]):
        cluster_points = X[labels == k]
        distances = np.linalg.norm(cluster_points - centroids[k], axis=1)
        closest_indices = np.argsort(distances)[:num_samples]
        selected_samples.append(cluster_points[closest_indices])
    return np.vstack(selected_samples)

# Logistic Regression using gradient descent
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_loss(X, y, weights):
    m = X.shape[0]
    predictions = sigmoid(np.dot(X, weights))
    return -1/m * (np.dot(y, np.log(predictions)) + np.dot(1 - y, np.log(1 - predictions)))

def logistic_regression(X_train, y_train, learning_rate=0.01, max_iters=1000):
    m, n = X_train.shape
    weights = np.zeros(n)
    for _ in range(max_iters):
        predictions = sigmoid(np.dot(X_train, weights))
        gradient = np.dot(X_train.T, predictions - y_train) / m
        weights -= learning_rate * gradient
    return weights

# Evaluation function
def evaluate_model(X_test, y_test, weights):
    predictions = sigmoid(np.dot(X_test, weights)) >= 0.5
    accuracy = np.mean(predictions == y_test)
    return accuracy

# Main function to run the entire pipeline
def run_kmeans_and_logistic_regression(iris_data_path, K):
    # Load and prepare the data
    X = load_iris_data(iris_data_path)
    
    # Run K-means clustering
    centroids, labels = kmeans(X, K)
    
    # Select 25 closest samples from each cluster
    X_selected = select_closest_samples(X, labels, centroids)
    
    # Remaining samples form the test set
    X_test = np.array([x for i, x in enumerate(X) if x not in X_selected])
    
    # For simplicity, we'll assume class labels as dummy for now
    # Training Logistic Regression and evaluating model
    y_train = np.random.randint(0, 2, size=X_selected.shape[0])  # Dummy labels for logistic regression
    y_test = np.random.randint(0, 2, size=X_test.shape[0])        # Dummy labels for logistic regression
    
    # Train logistic regression model
    weights = logistic_regression(X_selected, y_train)
    
    # Evaluate the model on the test set
    accuracy = evaluate_model(X_test, y_test, weights)
    return accuracy

# Example usage:
accuracy = run_kmeans_and_logistic_regression("iris.csv", 3)
print(f"Model accuracy: {accuracy:.2f}")
